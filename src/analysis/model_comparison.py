"""
æ¨¡å‹å¯¹æ¯”åˆ†ææ¨¡å—

æä¾›åŸºå‡†æ¨¡å‹ä¸ç”¨æˆ·æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å¯¹æ¯”åˆ†æåŠŸèƒ½ã€‚
"""

from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import json
from datetime import datetime

import numpy as np

from src.models.base import BaseModel
from src.models import create_model, UserModelLoader
from src.data.coco_dataset import COCOInferenceDataset
from src.metrics.coco_metrics import COCOMetrics, PerformanceMetrics


class ModelComparison:
    """æ¨¡å‹å¯¹æ¯”åˆ†æç±»"""

    def __init__(
        self,
        baseline_model: BaseModel,
        user_model: BaseModel,
        logger,
    ):
        self.baseline_model = baseline_model
        self.user_model = user_model
        self.logger = logger

        self.baseline_results = None
        self.user_results = None
        self.comparison = None

    def run_comparison(
        self,
        dataset: COCOInferenceDataset,
        annotations_file: str,
        max_images: Optional[int] = None,
        conf_threshold: float = 0.001,
    ) -> None:
        """
        è¿è¡Œæ¨¡å‹å¯¹æ¯”

        Args:
            dataset: æ•°æ®é›†
            annotations_file: COCO æ ‡æ³¨æ–‡ä»¶è·¯å¾„
            max_images: æœ€å¤§å›¾ç‰‡æ•°é‡
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        """
        self.logger.info("=" * 70)
        self.logger.info("å¼€å§‹æ¨¡å‹å¯¹æ¯”åˆ†æ")
        self.logger.info("=" * 70)

        # è¿è¡ŒåŸºå‡†æ¨¡å‹
        self.logger.info(
            f"è¯„ä¼°åŸºå‡†æ¨¡å‹: {self.baseline_model.model_info.get('name', 'Unknown')}"
        )
        self.baseline_results = self._evaluate_model(
            self.baseline_model, dataset, annotations_file, max_images, conf_threshold
        )

        if self.baseline_results is None:
            self.logger.error("åŸºå‡†æ¨¡å‹è¯„ä¼°å¤±è´¥")
            return

        # è¿è¡Œç”¨æˆ·æ¨¡å‹
        self.logger.info(
            f"è¯„ä¼°ç”¨æˆ·æ¨¡å‹: {self.user_model.model_info.get('name', 'Unknown')}"
        )
        self.user_results = self._evaluate_model(
            self.user_model, dataset, annotations_file, max_images, conf_threshold
        )

        if self.user_results is None:
            self.logger.error("ç”¨æˆ·æ¨¡å‹è¯„ä¼°å¤±è´¥")
            return

        # ç”Ÿæˆå¯¹æ¯”åˆ†æ
        self.logger.info("ç”Ÿæˆå¯¹æ¯”åˆ†æ...")
        self.comparison = self._generate_comparison()

        # è¾“å‡ºå¯¹æ¯”ç»“æœ
        self._log_comparison_results()

    def _evaluate_model(
        self,
        model: BaseModel,
        dataset: COCOInferenceDataset,
        annotations_file: str,
        max_images: Optional[int],
        conf_threshold: float,
    ) -> Optional[Dict[str, Any]]:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        try:
            coco_metrics_calculator = COCOMetrics(annotations_file)
            perf_metrics = PerformanceMetrics()

            total_images = max_images if max_images else len(dataset)
            self.logger.info(f"å°†å¤„ç† {total_images} å¼ å›¾ç‰‡")

            all_detections = {}

            for idx, (image_id, image) in enumerate(dataset):
                if idx >= total_images:
                    break

                try:
                    detections = model.predict(image, conf_threshold)

                    if detections:
                        all_detections[image_id] = detections

                except Exception as e:
                    self.logger.warning(f"  å›¾ç‰‡ {idx} æ¨ç†å¤±è´¥: {e}")
                    continue

            # è®¡ç®— COCO æŒ‡æ ‡
            predictions = coco_metrics_calculator.predictions_to_coco_format(
                all_detections
            )
            coco_metrics = coco_metrics_calculator.compute_metrics(predictions)

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            performance_stats = perf_metrics.compute_performance_stats()

            model_info = model.get_model_info()

            return {
                "model_name": model_info.get("name", "Unknown"),
                "coco_metrics": coco_metrics,
                "performance": performance_stats,
                "model_info": model_info,
                "num_images": total_images,
                "num_detections": len(all_detections),
            }

        except Exception as e:
            self.logger.error(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return None

    def _generate_comparison(self) -> Dict[str, Any]:
        """ç”Ÿæˆå¯¹æ¯”åˆ†æç»“æœ"""
        baseline = self.baseline_results
        user = self.user_results

        # åŸºç¡€æŒ‡æ ‡å¯¹æ¯”
        comparison = {
            "baseline_name": baseline["model_name"],
            "user_model_name": user["model_name"],
            "timestamp": datetime.now().isoformat(),
            "metrics_comparison": {},
            "performance_comparison": {},
            "recommendations": [],
        }

        # mAP å¯¹æ¯”
        for metric in ["AP@0.50", "AP@0.50:0.95", "AP@0.50:0.05", "AP@0.75"]:
            baseline_val = baseline["coco_metrics"].get(metric, 0)
            user_val = user["coco_metrics"].get(metric, 0)

            diff = user_val - baseline_val
            diff_pct = (diff / baseline_val * 100) if baseline_val > 0 else 0

            comparison["metrics_comparison"][metric] = {
                "baseline": baseline_val,
                "user": user_val,
                "diff": diff,
                "diff_pct": diff_pct,
            }

        # æ€§èƒ½å¯¹æ¯”
        baseline_fps = baseline["performance"].get("fps", 0)
        user_fps = user["performance"].get("fps", 0)

        comparison["performance_comparison"] = {
            "baseline_fps": baseline_fps,
            "user_fps": user_fps,
            "fps_diff": user_fps - baseline_fps,
            "fps_diff_pct": ((user_fps - baseline_fps) / baseline_fps * 100)
            if baseline_fps > 0
            else 0,
            "speedup": user_fps / baseline_fps if baseline_fps > 0 else 0,
            "baseline_avg_time": baseline["performance"].get("avg_time_ms", 0),
            "user_avg_time": user["performance"].get("avg_time_ms", 0),
        }

        # ç”Ÿæˆæ¨è
        comparison["recommendations"] = self._generate_recommendations(comparison)

        return comparison

    def _generate_recommendations(self, comparison: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä½¿ç”¨æ¨è"""
        recommendations = []

        # å‡†ç¡®æ€§åˆ†æ
        map_50_diff = comparison["metrics_comparison"]["AP@0.50"]["diff"]
        map_diff = comparison["metrics_comparison"]["AP@0.50:0.95"]["diff"]

        if map_diff > 0.05:  # ç”¨æˆ·æ¨¡å‹æ›´å¥½
            recommendations.append("âœ… ç”¨æˆ·æ¨¡å‹åœ¨ mAP@0.50 ä¸Šæœ‰æ˜æ˜¾æå‡")
        elif map_diff > 0:
            recommendations.append("âœ… ç”¨æˆ·æ¨¡å‹åœ¨ mAP@0.50 ä¸Šæœ‰æ‰€æå‡")
        elif map_diff < -0.05:  # åŸºå‡†æ¨¡å‹æ›´å¥½
            recommendations.append("âš ï¸  ç”¨æˆ·æ¨¡å‹åœ¨ mAP@0.50 ä¸Šæ˜æ˜¾ä½äºåŸºå‡†")
        elif map_diff < -0.01:
            recommendations.append("âš ï¸  ç”¨æˆ·æ¨¡å‹åœ¨ mAP@0.50 ä¸Šç•¥ä½äºåŸºå‡†")

        # é€Ÿåº¦åˆ†æ
        fps_diff_pct = comparison["performance_comparison"]["fps_diff_pct"]
        speedup = comparison["performance_comparison"]["speedup"]

        if fps_diff_pct > 50:  # ç”¨æˆ·æ¨¡å‹å¿«å¾ˆå¤š
            recommendations.append(f"âœ… ç”¨æˆ·æ¨¡å‹é€Ÿåº¦å¿« {fps_diff_pct:.1f}%")
            recommendations.append(f"âœ… ç”¨æˆ·æ¨¡å‹é€Ÿåº¦æ˜¯åŸºå‡†çš„ {speedup:.2f}x")
        elif fps_diff_pct > 10:  # ç”¨æˆ·æ¨¡å‹æ˜æ˜¾æ›´å¿«
            recommendations.append(f"âœ… ç”¨æˆ·æ¨¡å‹é€Ÿåº¦å¿« {fps_diff_pct:.1f}%")
        elif fps_diff_pct < -50:  # ç”¨æˆ·æ¨¡å‹æ…¢å¾ˆå¤š
            recommendations.append(f"âš ï¸  ç”¨æˆ·æ¨¡å‹é€Ÿåº¦æ…¢ {abs(fps_diff_pct):.1f}%")
            recommendations.append(f"âš ï¸  ç”¨æˆ·æ¨¡å‹é€Ÿåº¦æ˜¯åŸºå‡†çš„ {1 / speedup:.2f}x")
        elif fps_diff_pct < -10:  # ç”¨æˆ·æ¨¡å‹æ˜æ˜¾æ›´æ…¢
            recommendations.append(f"âš ï¸  ç”¨æˆ·æ¨¡å‹é€Ÿåº¦æ…¢ {abs(fps_diff_pct):.1f}%")

        # æ¨¡å‹å¤§å°åˆ†æ
        baseline_params = self.baseline_results["model_info"].get("params", 0)
        user_params = self.user_results["model_info"].get("params", 0)

        if baseline_params > 0:
            param_ratio = user_params / baseline_params
            if param_ratio < 0.8:  # ç”¨æˆ·æ¨¡å‹æ˜æ˜¾æ›´å°
                recommendations.append(f"âœ… ç”¨æˆ·æ¨¡å‹å‚æ•°é‡æ›´å°‘ ({param_ratio:.2f}x)")
            elif param_ratio > 1.2:  # ç”¨æˆ·æ¨¡å‹æ˜æ˜¾æ›´å¤§
                recommendations.append(f"âš ï¸  ç”¨æˆ·æ¨¡å‹å‚æ•°é‡æ›´å¤š ({param_ratio:.2f}x)")

        # ç»¼åˆæ¨è
        if map_diff > 0 and fps_diff_pct > 0:
            recommendations.append("ğŸ‰ ç”¨æˆ·æ¨¡å‹åœ¨å‡†ç¡®ç‡å’Œé€Ÿåº¦ä¸Šéƒ½ä¼˜äºåŸºå‡†")
        elif map_diff > 0:
            recommendations.append("âœ… ç”¨æˆ·æ¨¡å‹å‡†ç¡®ç‡æ›´é«˜ï¼Œå»ºè®®é‡‡ç”¨")
        elif map_diff < 0 and fps_diff_pct > 0:
            recommendations.append("âš–ï¸  æƒè¡¡ï¼šç”¨æˆ·æ¨¡å‹æ›´å¿«ä½†å‡†ç¡®ç‡ç•¥ä½")
        elif map_diff < 0:
            recommendations.append("âŒ ç”¨æˆ·æ¨¡å‹å‡†ç¡®ç‡ä½äºåŸºå‡†ï¼Œéœ€è¦æ”¹è¿›")

        return recommendations

    def _log_comparison_results(self) -> None:
        """è¾“å‡ºå¯¹æ¯”ç»“æœ"""
        if not self.comparison:
            return

        comp = self.comparison

        self.logger.info("")
        self.logger.info("=" * 70)
        self.logger.info("æ¨¡å‹å¯¹æ¯”ç»“æœ")
        self.logger.info("=" * 70)

        # åŸºæœ¬ä¿¡æ¯
        self.logger.info(f"åŸºå‡†æ¨¡å‹: {comp['baseline_name']}")
        self.logger.info(f"ç”¨æˆ·æ¨¡å‹: {comp['user_model_name']}")
        self.logger.info(f"æµ‹è¯•æ—¶é—´: {comp['timestamp']}")

        # mAP å¯¹æ¯”
        self.logger.info("")
        self.logger.info("mAP æŒ‡æ ‡å¯¹æ¯”:")
        self.logger.info("-" * 70)

        for metric in ["AP@0.50", "AP@0.50:0.95"]:
            baseline = comp["metrics_comparison"][metric]["baseline"]
            user = comp["metrics_comparison"][metric]["user"]
            diff = comp["metrics_comparison"][metric]["diff"]
            diff_pct = comp["metrics_comparison"][metric]["diff_pct"]

            baseline_str = f"{baseline:.4f}"
            user_str = f"{user:.4f}"
            diff_str = f"{diff:+.4f} ({diff_pct:+.2f}%)"

            self.logger.info(
                f"  {metric:20s} | åŸºå‡†: {baseline_str} | ç”¨æˆ·: {user_str} | å·®å¼‚: {diff_str}"
            )

        # æ€§èƒ½å¯¹æ¯”
        self.logger.info("")
        self.logger.info("æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”:")
        self.logger.info("-" * 70)

        perf = comp["performance_comparison"]
        self.logger.info(f"  åŸºå‡† FPS:     {perf['baseline_fps']:.2f}")
        self.logger.info(f"  ç”¨æˆ· FPS:     {perf['user_fps']:.2f}")
        self.logger.info(f"  FPS å·®å¼‚:     {perf['fps_diff']:+.2f}")
        self.logger.info(f"  FPS æå‡:      {perf['fps_diff_pct']:+.2f}%")
        self.logger.info(f"  åŠ é€Ÿæ¯”:       {perf['speedup']:.2f}x")
        self.logger.info(f"  åŸºå‡†å¹³å‡æ—¶é—´: {perf['baseline_avg_time']:.2f}ms")
        self.logger.info(f"  ç”¨æˆ·å¹³å‡æ—¶é—´: {perf['user_avg_time']:.2f}ms")

        # æ¨è
        self.logger.info("")
        self.logger.info("æ¨è:")
        self.logger.info("-" * 70)

        for rec in comp["recommendations"]:
            self.logger.info(f"  {rec}")

        self.logger.info("")
        self.logger.info("=" * 70)

    def save_results(
        self, output_path: Path, formats: List[str] = ["json", "html"]
    ) -> None:
        """
        ä¿å­˜å¯¹æ¯”ç»“æœ

        Args:
            output_path: è¾“å‡ºç›®å½•
            formats: è¾“å‡ºæ ¼å¼åˆ—è¡¨ï¼ˆjson, html, csvï¼‰
        """
        if not self.comparison:
            self.logger.warning("æ²¡æœ‰å¯¹æ¯”ç»“æœå¯ä¿å­˜")
            return

        output_path.mkdir(parents=True, exist_ok=True)

        # JSON
        if "json" in formats:
            json_file = output_path / "comparison.json"
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(self.comparison, f, indent=2, ensure_ascii=False)
            self.logger.info(f"JSON ç»“æœå·²ä¿å­˜: {json_file}")

        # HTML
        if "html" in formats:
            html_file = output_path / "comparison.html"
            html_content = self._generate_html_report()
            with open(html_file, "w", encoding="utf-8") as f:
                f.write(html_content)
            self.logger.info(f"HTML æŠ¥å‘Šå·²ä¿å­˜: {html_file}")

        # CSV
        if "csv" in formats:
            csv_file = output_path / "comparison.csv"
            csv_content = self._generate_csv_report()
            with open(csv_file, "w", encoding="utf-8") as f:
                f.write(csv_content)
            self.logger.info(f"CSV æŠ¥å‘Šå·²ä¿å­˜: {csv_file}")

    def _generate_html_report(self) -> str:
        """ç”Ÿæˆ HTML æŠ¥å‘Š"""
        if not self.comparison:
            return ""

        comp = self.comparison
        baseline = self.baseline_results
        user = self.user_results

        html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¨¡å‹å¯¹æ¯”åˆ†ææŠ¥å‘Š</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 20px;
            background-color: #f5f5f5;
            color: #333;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            border-radius: 8px;
        }}
        h1 {{
            color: #2c3e50;
            border-bottom: 3px solid #2c3e50;
            padding-bottom: 10px;
        }}
        h2 {{
            color: #34495e;
            border-bottom: 2px solid #34495e;
            padding-bottom: 8px;
            margin-top: 30px;
        }}
        .info-grid {{
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 20px;
            margin: 20px 0;
        }}
        .info-card {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            border-left: 4px solid #3498db;
        }}
        .metric-table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }}
        .metric-table th {{
            background: #343a40;
            color: white;
            padding: 12px;
            text-align: left;
        }}
        .metric-table td {{
            padding: 12px;
            border-bottom: 1px solid #dee2e6;
        }}
        .positive {{ color: #28a745; }}
        .negative {{ color: #dc3545; }}
        .recommendations {{
            background: #fff3cd;
            padding: 20px;
            border-radius: 6px;
            margin: 20px 0;
        }}
        .recommendation-item {{
            padding: 8px 0;
            border-bottom: 1px solid #ffeaa7;
        }}
        .recommendation-item:last-child {{
            border-bottom: none;
        }}
        .timestamp {{
            color: #6c757d;
            font-style: italic;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“Š æ¨¡å‹å¯¹æ¯”åˆ†ææŠ¥å‘Š</h1>
        <p class="timestamp">ç”Ÿæˆæ—¶é—´: {comp["timestamp"]}</p>

        <div class="info-grid">
            <div class="info-card">
                <h3>åŸºå‡†æ¨¡å‹</h3>
                <p><strong>åç§°:</strong> {comp["baseline_name"]}</p>
                <p><strong>mAP@0.50:</strong> {baseline["coco_metrics"]["AP@0.50"]:.4f}</p>
                <p><strong>mAP@0.50:0.95:</strong> {baseline["coco_metrics"]["AP@0.50:0.95"]:.4f}</p>
                <p><strong>FPS:</strong> {baseline["performance"]["fps"]:.2f}</p>
                <p><strong>å‚æ•°é‡:</strong> {baseline["model_info"].get("params", 0):.2f}M</p>
            </div>
            <div class="info-card">
                <h3>ç”¨æˆ·æ¨¡å‹</h3>
                <p><strong>åç§°:</strong> {comp["user_model_name"]}</p>
                <p><strong>mAP@0.50:</strong> {user["coco_metrics"]["AP@0.50"]:.4f}</p>
                <p><strong>mAP@0.50:0.95:</strong> {user["coco_metrics"]["AP@0.50:0.95"]:.4f}</p>
                <p><strong>FPS:</strong> {user["performance"]["fps"]:.2f}</p>
                <p><strong>å‚æ•°é‡:</strong> {user["model_info"].get("params", 0):.2f}M</p>
            </div>
        </div>

        <h2>ğŸ“ˆ æŒ‡æ ‡å¯¹æ¯”</h2>
        <table class="metric-table">
            <thead>
                <tr>
                    <th>æŒ‡æ ‡</th>
                    <th>åŸºå‡†æ¨¡å‹</th>
                    <th>ç”¨æˆ·æ¨¡å‹</th>
                    <th>å·®å¼‚</th>
                    <th>å˜åŒ– %</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>mAP@0.50</strong></td>
                    <td>{comp["metrics_comparison"]["AP@0.50"]["baseline"]:.4f}</td>
                    <td>{comp["metrics_comparison"]["AP@0.50"]["user"]:.4f}</td>
                    <td class="{"positive" if comp["metrics_comparison"]["AP@0.50"]["diff"] > 0 else "negative"}">{comp["metrics_comparison"]["AP@0.50"]["diff"]:+.4f}</td>
                    <td class="{"positive" if comp["metrics_comparison"]["AP@0.50"]["diff_pct"] > 0 else "negative"}">{comp["metrics_comparison"]["AP@0.50"]["diff_pct"]:+.2f}%</td>
                </tr>
                <tr>
                    <td><strong>mAP@0.50:0.95</strong></td>
                    <td>{comp["metrics_comparison"]["AP@0.50:0.95"]["baseline"]:.4f}</td>
                    <td>{comp["metrics_comparison"]["AP@0.50:0.95"]["user"]:.4f}</td>
                    <td class="{"positive" if comp["metrics_comparison"]["AP@0.50:0.95"]["diff"] > 0 else "negative"}">{comp["metrics_comparison"]["AP@0.50:0.95"]["diff"]:+.4f}</td>
                    <td class="{"positive" if comp["metrics_comparison"]["AP@0.50:0.95"]["diff_pct"] > 0 else "negative"}">{comp["metrics_comparison"]["AP@0.50:0.95"]["diff_pct"]:+.2f}%</td>
                </tr>
                <tr>
                    <td><strong>FPS</strong></td>
                    <td>{comp["performance_comparison"]["baseline_fps"]:.2f}</td>
                    <td>{comp["performance_comparison"]["user_fps"]:.2f}</td>
                    <td class="{"positive" if comp["performance_comparison"]["fps_diff"] > 0 else "negative"}">{comp["performance_comparison"]["fps_diff"]:+.2f}</td>
                    <td class="{"positive" if comp["performance_comparison"]["fps_diff_pct"] > 0 else "negative"}">{comp["performance_comparison"]["fps_diff_pct"]:+.2f}%</td>
                </tr>
                <tr>
                    <td><strong>åŠ é€Ÿæ¯”</strong></td>
                    <td>1.00x</td>
                    <td>{comp["performance_comparison"]["speedup"]:.2f}x</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
            </tbody>
        </table>

        <h2>ğŸ’¡ æ¨èä¸å»ºè®®</h2>
        <div class="recommendations">
"""

        for rec in comp["recommendations"]:
            html += f'            <div class="recommendation-item">{rec}</div>\n'

        html += f"""        </div>

        <p style="text-align: center; color: #6c757d; font-style: italic; margin-top: 40px;">
            æŠ¥å‘Šç”± od-benchmark ç”Ÿæˆ
        </p>
    </div>
</body>
</html>
"""

        return html

    def _generate_csv_report(self) -> str:
        """ç”Ÿæˆ CSV æŠ¥å‘Š"""
        if not self.comparison:
            return ""

        comp = self.comparison

        lines = []
        lines.append("æŒ‡æ ‡,åŸºå‡†æ¨¡å‹,ç”¨æˆ·æ¨¡å‹,å·®å¼‚,å˜åŒ–%")
        lines.append("-" * 60)

        for metric in ["AP@0.50", "AP@0.50:0.95"]:
            baseline = comp["metrics_comparison"][metric]["baseline"]
            user = comp["metrics_comparison"][metric]["user"]
            diff = comp["metrics_comparison"][metric]["diff"]
            diff_pct = comp["metrics_comparison"][metric]["diff_pct"]
            lines.append(
                f"{metric},{baseline:.4f},{user:.4f},{diff:+.4f},{diff_pct:+.2f}%"
            )

        lines.append("")
        lines.append("æ€§èƒ½æŒ‡æ ‡")
        lines.append(f"åŸºå‡† FPS,{comp['performance_comparison']['baseline_fps']:.2f}")
        lines.append(f"ç”¨æˆ· FPS,{comp['performance_comparison']['user_fps']:.2f}")
        lines.append(f"FPS å·®å¼‚,{comp['performance_comparison']['fps_diff']:+.2f}")
        lines.append(f"FPS æå‡,{comp['performance_comparison']['fps_diff_pct']:+.2f}%")
        lines.append(f"åŠ é€Ÿæ¯”,{comp['performance_comparison']['speedup']:.2f}x")

        lines.append("")
        lines.append("æ¨è")
        for rec in comp["recommendations"]:
            lines.append(rec)

        return "\n".join(lines)

    def get_comparison(self) -> Dict[str, Any]:
        """è·å–å¯¹æ¯”ç»“æœ"""
        return self.comparison if self.comparison else {}
