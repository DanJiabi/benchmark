"""
Command line interface for od-benchmark package
"""

import argparse
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import json

import numpy as np
from tqdm import tqdm

from src.models.base import Detection
from src.models import (
    create_model,
    load_model_wrapper,
    UserModelLoader,
    export_model_cli,
    batch_export_models,
)
from src.data.coco_dataset import COCOInferenceDataset
from src.metrics.coco_metrics import COCOMetrics, PerformanceMetrics, MetricsAggregator
from src.utils.logger import Config, setup_logger, download_model_weights
from src.utils.visualization import (
    save_detection_visualization,
    plot_metrics_comparison,
    plot_fps_vs_map,
    plot_model_size_vs_performance,
    generate_results_table,
)
from src.analysis import ModelComparison


def run_single_model(
    model_config: Dict[str, Any],
    dataset: COCOInferenceDataset,
    coco_metrics_calculator: COCOMetrics,
    logger,
    max_images: Optional[int] = None,
    conf_threshold: float = 0.001,
    visualize: bool = False,
    vis_dir: Optional[Path] = None,
    num_viz_images: int = 10,
) -> Optional[Dict[str, Any]]:
    model_name = model_config["name"]
    framework = model_config["framework"]
    weights_file = model_config["weights"]
    weights_url = model_config.get("url")

    logger.info(f"å¼€å§‹è¯„ä¼°æ¨¡å‹: {model_name}")

    try:
        model = create_model(model_name, device="auto", conf_threshold=conf_threshold)
    except ValueError as e:
        logger.error(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_name}")
        logger.error(f"   é”™è¯¯: {e}")
        return None

    weights_path = None
    if weights_file:
        weights_path = Path("models_cache") / weights_file
        if weights_url and not weights_path.exists():
            logger.info(f"ä¸‹è½½æ¨¡å‹æƒé‡: {weights_url}")
            try:
                download_model_weights(weights_url, weights_path)
            except Exception as e:
                logger.error(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {model_name}")
                logger.error(f"   URL: {weights_url}")
                logger.error(f"   é”™è¯¯: {e}")
                logger.warning(f"   è·³è¿‡è¯¥æ¨¡å‹ï¼Œç»§ç»­æµ‹è¯•å…¶ä»–æ¨¡å‹")
                return None

    logger.info(
        f"åŠ è½½æ¨¡å‹æƒé‡: {weights_path if weights_path else 'ä½¿ç”¨å†…ç½®é¢„è®­ç»ƒæƒé‡'}"
    )

    try:
        if weights_path:
            load_model_wrapper(model, str(weights_path), model_name)
        else:
            model.load_model(None)
    except FileNotFoundError:
        logger.error(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {weights_path}")
        logger.error("   è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æˆ–å…ˆä¸‹è½½æ¨¡å‹æƒé‡")
        return None
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        logger.error(f"   æ¨¡å‹: {model_name}")
        logger.error(f"   æƒé‡æ–‡ä»¶: {weights_path}")
        return None

    model_info = model.get_model_info()
    logger.info(f"æ¨¡å‹ä¿¡æ¯: {model_info}")

    logger.info("æ¨¡å‹é¢„çƒ­...")
    try:
        model.warmup()
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}")
        logger.warning("   ç»§ç»­æ‰§è¡Œï¼Œä½†é¦–æ¬¡æ¨ç†å¯èƒ½è¾ƒæ…¢")

    all_detections = {}
    perf_metrics = PerformanceMetrics()

    total_images = max_images if max_images else len(dataset)
    logger.info(f"å°†å¤„ç† {total_images} å¼ å›¾ç‰‡")

    image_iterator = enumerate(dataset)
    if total_images <= len(dataset):
        image_iterator = tqdm(
            image_iterator,
            total=total_images,
            desc=f"{model_name} æ¨ç†",
            unit="å¼ ",
            leave=False,
        )

    for idx, (image_id, image) in image_iterator:
        if idx >= total_images:
            break

        try:
            start_time = perf_metrics.start_timer()
            detections = model.predict(image, conf_threshold)
            inference_time = perf_metrics.end_timer(start_time)

            perf_metrics.add_inference_time(inference_time)
            all_detections[image_id] = detections

        except Exception as e:
            logger.error(
                f"âŒ æ¨ç†å¤±è´¥ (å›¾ç‰‡ {idx}/{total_images}, ID: {image_id}): {e}"
            )
            logger.warning("   è·³è¿‡æ­¤å›¾ç‰‡ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€å¼ ")
            continue

        if visualize and vis_dir and idx < num_viz_images and len(detections) > 0:
            viz_filename = f"{model_name}_vis_{idx:04d}_{image_id:012d}.jpg"
            viz_path = vis_dir / viz_filename

            class_names = model_info.get("model_yaml", {}).get("names", {})
            if not class_names and hasattr(model, "names"):
                class_names = model.names

            try:
                num_boxes = save_detection_visualization(
                    image, detections, class_names, viz_path
                )
                if idx == 0 or idx % 5 == 0:
                    logger.info(
                        f"    å·²ä¿å­˜å¯è§†åŒ–: {viz_filename} ({num_boxes} ä¸ªæ£€æµ‹æ¡†)"
                    )
            except Exception as e:
                logger.error(f"âŒ å¯è§†åŒ–å¤±è´¥: {viz_filename}")
                logger.error(f"   é”™è¯¯: {e}")

    logger.info("ç”Ÿæˆé¢„æµ‹ç»“æœ...")
    try:
        predictions = coco_metrics_calculator.predictions_to_coco_format(all_detections)
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆé¢„æµ‹ç»“æœå¤±è´¥: {e}")
        logger.error(f"   æ£€æµ‹æ•°é‡: {len(all_detections)}")
        return None

    logger.info("è®¡ç®— COCO æŒ‡æ ‡...")
    try:
        coco_metrics = coco_metrics_calculator.compute_metrics(predictions)
    except Exception as e:
        logger.error(f"âŒ è®¡ç®— COCO æŒ‡æ ‡å¤±è´¥: {e}")
        logger.error("   è¯·æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼")
        return None

    performance_stats = perf_metrics.compute_performance_stats()

    logger.info(f"{model_name} æŒ‡æ ‡:")
    logger.info(f"  AP@0.50: {coco_metrics['AP@0.50']:.4f}")
    logger.info(f"  AP@0.50:0.95: {coco_metrics['AP@0.50:0.95']:.4f}")
    logger.info(f"  FPS: {performance_stats['fps']:.2f}")

    result = {
        "model_name": model_name,
        "framework": framework,
        "coco_metrics": coco_metrics,
        "performance": performance_stats,
        "model_info": model_info,
    }

    return result


def benchmark_main():
    """Main benchmark function"""
    parser = argparse.ArgumentParser(description="ç›®æ ‡æ£€æµ‹æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
    parser.add_argument(
        "--config", type=str, default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument("--model", type=str, action="append", help="æŒ‡å®šè¦æµ‹è¯•çš„æ¨¡å‹")
    parser.add_argument("--all", action="store_true", help="æµ‹è¯•æ‰€æœ‰é…ç½®çš„æ¨¡å‹")
    parser.add_argument(
        "--output-dir", type=str, default="outputs/results", help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--visualize",
        action="store_true",
        help="ä¿å­˜æ£€æµ‹æ¡†å¯è§†åŒ–å›¾ç‰‡",
    )
    parser.add_argument(
        "--num-viz-images",
        type=int,
        default=10,
        help="å¯è§†åŒ–å›¾ç‰‡æ•°é‡ï¼ˆé»˜è®¤: 10ï¼‰",
    )
    parser.add_argument(
        "--conf-threshold",
        type=float,
        default=None,
        help="ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé»˜è®¤: ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰",
    )
    parser.add_argument(
        "--num-images",
        type=int,
        default=None,
        help="æµ‹è¯•å›¾ç‰‡æ•°é‡ï¼ˆé»˜è®¤: å…¨éƒ¨æ•°æ®ï¼‰",
    )

    args = parser.parse_args()

    try:
        config = Config(args.config)
        logger = setup_logger(config)
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        print("   è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶è·¯å¾„")
        return
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        print(f"   æ–‡ä»¶: {args.config}")
        return

    logger.info("=" * 60)
    logger.info("ç›®æ ‡æ£€æµ‹æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
    logger.info("=" * 60)

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    vis_dir = None
    if args.visualize:
        vis_dir = output_dir.parent / "visualizations"
        vis_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"å¯è§†åŒ–ç›®å½•: {vis_dir}")

    dataset_config = config.get_dataset_config()
    dataset_path = dataset_config["path"]
    split = dataset_config["split"]

    logger.info(f"åŠ è½½æ•°æ®é›†: {dataset_path}/{split}")
    dataset = COCOInferenceDataset(dataset_path, split)
    logger.info(f"æ•°æ®é›†å¤§å°: {len(dataset)} å¼ å›¾ç‰‡")

    annotations_file = (
        Path(dataset_path).expanduser() / "annotations" / f"instances_{split}.json"
    )
    coco_metrics_calculator = COCOMetrics(str(annotations_file))

    models_config = config.get_models_config()
    eval_config = config.get_evaluation_config()
    test_config = config.config.get("test", {})
    max_images = (
        args.num_images
        if args.num_images is not None
        else test_config.get("max_images")
    )

    conf_threshold = args.conf_threshold
    if conf_threshold is None:
        conf_threshold = eval_config.get("conf_threshold", 0.001)
        logger.info(f"ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold}")
    else:
        logger.info(f"ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold}")

    models_to_test = []
    if args.all:
        models_to_test = models_config
    elif args.model:
        for model_name in args.model:
            if model_name.lower() == "all":
                models_to_test = models_config
                break
            for model_cfg in models_config:
                if model_cfg["name"] == model_name:
                    models_to_test.append(model_cfg)
                    break
    else:
        logger.error("è¯·ä½¿ç”¨ --model <model_name> æˆ– --all æŒ‡å®šè¦æµ‹è¯•çš„æ¨¡å‹")
        logger.info("å¯ç”¨çš„æ¨¡å‹: " + ", ".join([m["name"] for m in models_config]))
        return

    if not models_to_test:
        logger.error("æœªæ‰¾åˆ°è¦æµ‹è¯•çš„æ¨¡å‹")
        return

    logger.info(f"è®¡åˆ’æµ‹è¯• {len(models_to_test)} ä¸ªæ¨¡å‹")

    aggregator = MetricsAggregator()

    for model_config in tqdm(models_to_test, desc="æ¨¡å‹è¿›åº¦", unit="æ¨¡å‹"):
        result = run_single_model(
            model_config,
            dataset,
            coco_metrics_calculator,
            logger,
            max_images,
            conf_threshold,
            args.visualize,
            vis_dir,
            args.num_viz_images,
        )

        if result:
            aggregator.add_model_result(
                result["model_name"],
                result["coco_metrics"],
                result["performance"],
                result["model_info"],
            )

            import json

            result_file = output_dir / f"{result['model_name']}_result.json"
            try:
                with open(result_file, "w") as f:
                    json.dump(result, f, indent=2)
                logger.info(f"ç»“æœå·²ä¿å­˜: {result_file}")
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜ç»“æœæ–‡ä»¶å¤±è´¥: {result_file}")
                logger.error(f"   é”™è¯¯: {e}")

    logger.info("=" * 60)
    logger.info("ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
    logger.info("=" * 60)

    all_results = aggregator.get_all_results()

    if not all_results:
        logger.warning("æ²¡æœ‰å¯ç”¨çš„ç»“æœç”¨äºç”ŸæˆæŠ¥å‘Š")
        logger.info("=" * 60)
        logger.info("åŸºå‡†æµ‹è¯•å®Œæˆï¼")
        logger.info("=" * 60)
        return

    comparison_file = output_dir / "comparison.json"
    aggregator.save_results(str(comparison_file))
    logger.info(f"å¯¹æ¯”ç»“æœå·²ä¿å­˜: {comparison_file}")

    results_table = generate_results_table(all_results)
    logger.info("\n" + "=" * 60)
    logger.info("æ€§èƒ½å¯¹æ¯”è¡¨æ ¼")
    logger.info("=" * 60)
    logger.info(results_table.to_string())

    table_file = output_dir / "results_table.csv"
    results_table.to_csv(table_file)
    logger.info(f"è¡¨æ ¼å·²ä¿å­˜: {table_file}")

    figures_dir = output_dir.parent / "figures"
    figures_dir.mkdir(parents=True, exist_ok=True)

    try:
        plot_metrics_comparison(
            all_results,
            ["AP@0.50", "AP@0.50:0.95", "fps"],
            str(figures_dir / "metrics_comparison.png"),
        )
        logger.info(f"æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜: {figures_dir / 'metrics_comparison.png'}")
    except Exception as e:
        logger.error(f"âŒ ç”ŸæˆæŒ‡æ ‡å¯¹æ¯”å›¾å¤±è´¥: {e}")

    try:
        plot_fps_vs_map(all_results, str(figures_dir / "fps_vs_map.png"))
        logger.info(f"FPS vs mAP å›¾å·²ä¿å­˜: {figures_dir / 'fps_vs_map.png'}")
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆ FPS vs mAP å›¾å¤±è´¥: {e}")

    try:
        plot_model_size_vs_performance(
            all_results, str(figures_dir / "size_vs_performance.png")
        )
        logger.info(
            f"æ¨¡å‹å¤§å° vs æ€§èƒ½å›¾å·²ä¿å­˜: {figures_dir / 'size_vs_performance.png'}"
        )
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆæ¨¡å‹å¤§å° vs æ€§èƒ½å›¾å¤±è´¥: {e}")

    logger.info("=" * 60)
    logger.info("åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    logger.info("=" * 60)


def analyze_main(args=None):
    """Main analyze function"""
    # å¦‚æœ args ä¸º Noneï¼Œåˆ™è§£æå‚æ•°ï¼ˆç”¨äºç›´æ¥è°ƒç”¨ï¼‰
    if args is None:
        parser = argparse.ArgumentParser(
            description="Object Detection Benchmark - Model Analysis",
            epilog="Run 'od-benchmark analyze --help' for more information.",
        )

        parser.add_argument(
            "--baseline",
            type=str,
            action="append",
            help="Baseline model name(s) from config.yaml",
        )
        parser.add_argument(
            "--all-baselines",
            action="store_true",
            help="Use all configured baseline models",
        )
        parser.add_argument(
            "--user-model",
            type=str,
            action="append",
            required=True,
            help="User model(s)",
        )
        parser.add_argument(
            "--config",
            type=str,
            default="config.yaml",
            help="Configuration file path",
        )
        parser.add_argument(
            "--num-images",
            type=int,
            default=50,
            help="Number of test images",
        )
        parser.add_argument(
            "--output-dir",
            type=str,
            default="outputs/analysis",
            help="Output directory",
        )
        parser.add_argument(
            "--format",
            type=str,
            default="all",
            choices=["json", "html", "csv", "all"],
            help="Output format",
        )
        parser.add_argument(
            "--debug",
            action="store_true",
            help="Debug mode",
        )

        args = parser.parse_args()

    try:
        config = Config(args.config)
        logger = setup_logger(config)
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        print("   è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶è·¯å¾„")
        return
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        print(f"   æ–‡ä»¶: {args.config}")
        return

    logger.info("=" * 70)
    logger.info("æ¨¡å‹å¯¹æ¯”åˆ†æ")
    logger.info("=" * 70)

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # åŠ è½½é…ç½®è·å–æ¨¡å‹
    models_config = config.get_models_config()
    dataset_config = config.get_dataset_config()
    dataset_path = dataset_config["path"]
    split = dataset_config["split"]

    logger.info(f"åŠ è½½æ•°æ®é›†: {dataset_path}/{split}")
    dataset = COCOInferenceDataset(dataset_path, split)
    logger.info(f"æ•°æ®é›†å¤§å°: {len(dataset)} å¼ å›¾ç‰‡")

    # è·å–åŸºå‡†æ¨¡å‹åˆ—è¡¨
    baseline_models = []
    if args.all_baselines:
        baseline_models = models_config
        logger.info(f"ä½¿ç”¨æ‰€æœ‰åŸºå‡†æ¨¡å‹: {[m['name'] for m in baseline_models]}")
    elif args.baseline:
        baseline_names = args.baseline
        for baseline_name in baseline_names:
            for model_cfg in models_config:
                if model_cfg["name"] == baseline_name:
                    baseline_models.append(model_cfg)
                    break
        logger.info(f"ä½¿ç”¨åŸºå‡†æ¨¡å‹: {baseline_names}")
    else:
        logger.error("âŒ å¿…é¡»æŒ‡å®š --baseline æˆ– --all-baselines")
        logger.info(f"å¯ç”¨çš„æ¨¡å‹: {', '.join([m['name'] for m in models_config])}")
        return

    if not baseline_models:
        logger.error("âŒ æœªæ‰¾åˆ°åŸºå‡†æ¨¡å‹")
        logger.info(f"å¯ç”¨çš„æ¨¡å‹: {', '.join([m['name'] for m in models_config])}")
        return

    # è·å–ç”¨æˆ·æ¨¡å‹åˆ—è¡¨
    user_models = args.user_model if args.user_model else []
    logger.info(f"ç”¨æˆ·æ¨¡å‹: {user_models}")

    conf_threshold = 0.001

    # è·å–æ ‡æ³¨æ–‡ä»¶
    annotations_file = (
        Path(dataset_path).expanduser() / "annotations" / f"instances_{split}.json"
    )

    # è¿è¡Œæ‰€æœ‰å¯¹æ¯”
    from src.analysis import ModelComparison

    all_comparisons = []
    for baseline_config in baseline_models:
        baseline_name = baseline_config["name"]
        logger.info(f"åŠ è½½åŸºå‡†æ¨¡å‹: {baseline_name}")

        try:
            baseline_model = create_model(
                baseline_name, device="auto", conf_threshold=conf_threshold
            )
            weights_file = baseline_config.get("weights")
            if weights_file:
                from . import load_model_wrapper

                load_model_wrapper(
                    baseline_model,
                    str(Path("models_cache") / weights_file),
                    baseline_name,
                )
        except ValueError as e:
            logger.error(f"âŒ åŸºå‡†æ¨¡å‹ {baseline_name} åŠ è½½å¤±è´¥: {e}")
            continue

        # å¯¹æ¯ä¸ªç”¨æˆ·æ¨¡å‹è¿›è¡Œå¯¹æ¯”
        for user_model_spec in user_models:
            logger.info(f"  å¯¹æ¯”ç”¨æˆ·æ¨¡å‹: {user_model_spec}")

            try:
                user_model = UserModelLoader.load_user_model(
                    user_model_spec, device="auto", conf_threshold=conf_threshold
                )

                comparison = ModelComparison(baseline_model, user_model, logger)
                comparison.run_comparison(
                    dataset=dataset,
                    annotations_file=str(annotations_file),
                    max_images=args.num_images,
                    conf_threshold=conf_threshold,
                )

                comparison_result = comparison.get_comparison()
                comparison_result["baseline_name"] = baseline_name
                comparison_result["user_model_spec"] = user_model_spec
                comparison_result["timestamp"] = datetime.now().isoformat()

                all_comparisons.append(comparison_result)

            except Exception as e:
                logger.error(f"  âŒ ç”¨æˆ·æ¨¡å‹ {user_model_spec} å¯¹æ¯”å¤±è´¥: {e}")
                import traceback

                traceback.print_exc()
                continue

    if not all_comparisons:
        logger.error("âŒ æ²¡æœ‰æˆåŠŸçš„å¯¹æ¯”åˆ†æ")
        return

    # ä¿å­˜æ±‡æ€»ç»“æœ
    summary = {
        "timestamp": datetime.now().isoformat(),
        "num_comparisons": len(all_comparisons),
        "baseline_models": [m["name"] for m in baseline_models],
        "user_models": user_models,
        "comparisons": all_comparisons,
    }

    # ä¿å­˜æ‰€æœ‰ç»“æœ
    formats = []
    if args.format == "all":
        formats = ["json", "html", "csv"]
    else:
        formats = [args.format]

    # ä¿å­˜æ±‡æ€»
    summary_file = output_dir / "summary.json"
    with open(summary_file, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    logger.info(f"æ±‡æ€»ç»“æœå·²ä¿å­˜: {summary_file}")

    # ä¸ºæ¯ä¸ªå¯¹æ¯”ä¿å­˜å•ç‹¬ç»“æœ
    for idx, comp in enumerate(all_comparisons):
        comp_dir = output_dir / f"comparison_{idx:03d}"
        comp_dir.mkdir(parents=True, exist_ok=True)

        comp_file = comp_dir / "comparison.json"
        with open(comp_file, "w", encoding="utf-8") as f:
            json.dump(comp, f, indent=2, ensure_ascii=False)

        # HTML
        if "html" in formats:
            html_content = _generate_multi_model_html_report(comp)
            html_file = comp_dir / "comparison.html"
            with open(html_file, "w", encoding="utf-8") as f:
                f.write(html_content)

        # CSV
        if "csv" in formats:
            csv_content = _generate_multi_model_csv_report(comp)
            csv_file = comp_dir / "comparison.csv"
            with open(csv_file, "w", encoding="utf-8") as f:
                f.write(csv_content)

    logger.info("")
    logger.info("=" * 70)
    logger.info("æ‰€æœ‰å¯¹æ¯”åˆ†æå®Œæˆï¼")
    logger.info(f"æ±‡æ€»: {summary_file}")
    logger.info(f"å¯¹æ¯”ç»“æœ: {output_dir}")
    logger.info("=" * 70)


def _generate_multi_model_html_report(comparison: dict) -> str:
    """ç”Ÿæˆå¤šæ¨¡å‹å¯¹æ¯” HTML æŠ¥å‘Š"""
    if not comparison:
        return ""

    baseline = comparison.get("baseline_results", {})
    user = comparison.get("user_results", {})
    comp = comparison.get("comparison", {})

    html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¨¡å‹å¯¹æ¯”åˆ†ææŠ¥å‘Š</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 20px;
            background-color: #f5f5f5;
            color: #333;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            border-radius: 8px;
        }}
        .header {{
            border-bottom: 2px solid #2c3e50;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }}
        .info-grid {{
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 20px;
            margin: 20px 0;
        }}
        .info-card {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            border-left: 4px solid #3498db;
        }}
        .metrics-table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }}
        .metrics-table th {{
            background: #343a40;
            color: white;
            padding: 12px;
            text-align: left;
        }}
        .metrics-table td {{
            padding: 12px;
            border-bottom: 1px solid #dee2e6;
        }}
        .recommendations {{
            background: #fff3cd;
            padding: 20px;
            border-radius: 6px;
            margin: 20px 0;
        }}
        .recommendation-item {{
            padding: 8px 0;
            border-bottom: 1px solid #ffeaa7;
        }}
        .recommendation-item:last-child {{
            border-bottom: none;
        }}
        .positive {{ color: #28a745; }}
        .negative {{ color: #dc3545; }}
        .timestamp {{
            color: #6c757d;
            font-style: italic;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š æ¨¡å‹å¯¹æ¯”åˆ†ææŠ¥å‘Š</h1>
            <p class="timestamp">ç”Ÿæˆæ—¶é—´: {comparison.get("timestamp", "N/A")}</p>
        </div>

        <div class="info-grid">
            <div class="info-card">
                <h3>åŸºå‡†æ¨¡å‹</h3>
                <p><strong>åç§°:</strong> {comparison.get("baseline_name", "N/A")}</p>
            </div>
            <div class="info-card">
                <h3>ç”¨æˆ·æ¨¡å‹</h3>
                <p><strong>æ ‡è¯†:</strong> {comparison.get("user_model_spec", "N/A")}</p>
            </div>
        </div>

        <h2>ğŸ“ˆ å¯¹æ¯”ç»“æœ</h2>
        <table class="metrics-table">
            <thead>
                <tr>
                    <th>æŒ‡æ ‡</th>
                    <th>åŸºå‡†æ¨¡å‹</th>
                    <th>ç”¨æˆ·æ¨¡å‹</th>
                    <th>å·®å¼‚</th>
                    <th>å˜åŒ– %</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>mAP@0.50</strong></td>
                    <td>{baseline.get("mAP@0.50", 0):.4f}</td>
                    <td>{user.get("mAP@0.50", 0):.4f}</td>
                    <td class="{"positive" if comp.get("map_diff", 0) > 0 else "negative"}">{comp.get("map_diff", 0):+.4f}</td>
                    <td class="{"positive" if comp.get("map_diff_pct", 0) > 0 else "negative"}">{comp.get("map_diff_pct", 0):+.2f}%</td>
                </tr>
                <tr>
                    <td><strong>mAP@0.50:0.95</strong></td>
                    <td>{baseline.get("mAP@0.50:0.95", 0):.4f}</td>
                    <td>{user.get("mAP@0.50:0.95", 0):.4f}</td>
                    <td class="{"positive" if comp.get("map_diff_95", 0) > 0 else "negative"}">{comp.get("map_diff_95", 0):+.4f}</td>
                    <td class="{"positive" if comp.get("map_diff_95_pct", 0) > 0 else "negative"}">{comp.get("map_diff_95_pct", 0):+.2f}%</td>
                </tr>
            </tbody>
        </table>

        <h2>ğŸ’¡ å»ºè®®</h2>
        <div class="recommendations">
            <div class="recommendation-item">
                {comparison.get("recommendation", "N/A")}
            </div>
        </div>

        <p style="text-align: center; color: #6c757d; font-style: italic; margin-top: 40px;">
            æŠ¥å‘Šç”± od-benchmark ç”Ÿæˆ
        </p>
    </div>
</body>
</html>
"""
    return html


def _generate_multi_model_csv_report(comparison: dict) -> str:
    """ç”Ÿæˆå¤šæ¨¡å‹å¯¹æ¯” CSV æŠ¥å‘Š"""
    if not comparison:
        return ""

    comp = comparison.get("comparison", {})

    lines = []
    lines.append("æŒ‡æ ‡,åŸºå‡†æ¨¡å‹,ç”¨æˆ·æ¨¡å‹,å·®å¼‚,å˜åŒ–%")
    lines.append("-" * 50)

    lines.append(
        f"mAP@0.50,{comp.get('baseline_map_50', 0):.4f},{comp.get('user_map_50', 0):.4f},{comp.get('map_diff', 0):+.4f},{comp.get('map_diff_pct', 0):+.2f}%"
    )
    lines.append(
        f"mAP@0.50:0.95,{comp.get('baseline_map_95', 0):.4f},{comp.get('user_map_95', 0):.4f},{comp.get('map_diff_95', 0):+.4f},{comp.get('map_diff_95_pct', 0):+.2f}%"
    )
    lines.append("")
    lines.append(f"å»ºè®®,{comparison.get('recommendation', 'N/A')}")

    return "\n".join(lines)
    logger.info("æ¨¡å‹å¯¹æ¯”åˆ†æ")
    logger.info("=" * 70)

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # åŠ è½½é…ç½®è·å–åŸºå‡†æ¨¡å‹
    models_config = config.get_models_config()
    dataset_config = config.get_dataset_config()
    dataset_path = dataset_config["path"]
    split = dataset_config["split"]

    logger.info(f"åŠ è½½æ•°æ®é›†: {dataset_path}/{split}")
    dataset = COCOInferenceDataset(dataset_path, split)
    logger.info(f"æ•°æ®é›†å¤§å°: {len(dataset)} å¼ å›¾ç‰‡")

    # æŸ¥æ‰¾åŸºå‡†æ¨¡å‹
    baseline_config = None
    for model_cfg in models_config:
        if model_cfg["name"] == args.baseline:
            baseline_config = model_cfg
            break

    if not baseline_config:
        logger.error(f"âŒ æœªæ‰¾åˆ°åŸºå‡†æ¨¡å‹: {args.baseline}")
        logger.info(f"å¯ç”¨çš„æ¨¡å‹: {', '.join([m['name'] for m in models_config])}")
        return

    logger.info(f"åŸºå‡†æ¨¡å‹: {args.baseline}")
    logger.info(f"ç”¨æˆ·æ¨¡å‹: {args.user_model}")

    # åŠ è½½æ¨¡å‹
    conf_threshold = 0.001

    try:
        baseline_model = create_model(
            args.baseline, device="auto", conf_threshold=conf_threshold
        )
        weights_file = baseline_config.get("weights")
        if weights_file:
            from . import load_model_wrapper

            load_model_wrapper(
                baseline_model, str(Path("models_cache") / weights_file), args.baseline
            )
    except ValueError as e:
        logger.error(f"âŒ åŸºå‡†æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # åŠ è½½ç”¨æˆ·æ¨¡å‹
    try:
        user_model = UserModelLoader.load_user_model(
            args.user_model, device="auto", conf_threshold=conf_threshold
        )
    except Exception as e:
        logger.error(f"âŒ ç”¨æˆ·æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # åˆ›å»ºå¯¹æ¯”å™¨
    comparison = ModelComparison(baseline_model, user_model, logger)

    # è·å–æ ‡æ³¨æ–‡ä»¶
    annotations_file = (
        Path(dataset_path).expanduser() / "annotations" / f"instances_{split}.json"
    )

    # è¿è¡Œå¯¹æ¯”
    comparison.run_comparison(
        dataset=dataset,
        annotations_file=str(annotations_file),
        max_images=args.num_images,
        conf_threshold=conf_threshold,
    )

    # ä¿å­˜ç»“æœ
    formats = []
    if args.format == "all":
        formats = ["json", "html", "csv"]
    else:
        formats = [args.format]

    comparison.save_results(output_dir, formats)

    logger.info("")
    logger.info("=" * 70)
    logger.info("åˆ†æå®Œæˆï¼")
    logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    logger.info("=" * 70)


def main():
    parser = argparse.ArgumentParser(
        description="Object Detection Benchmark - Performance evaluation tool",
        epilog="Run 'od-benchmark <command> --help' for more information on a command.",
    )

    parser.add_argument("--version", action="version", version="od-benchmark 0.1.0")

    subparsers = parser.add_subparsers(
        dest="command", help="Available commands", required=False
    )

    # Benchmark command
    benchmark_parser = subparsers.add_parser(
        "benchmark",
        help="Run benchmark evaluation",
        description="Run object detection model benchmark evaluation",
    )
    benchmark_parser.add_argument(
        "--config",
        type=str,
        default="config.yaml",
        help="Configuration file path (default: config.yaml)",
    )
    benchmark_parser.add_argument(
        "--model",
        type=str,
        action="append",
        help="Specify model(s) to test (can be used multiple times)",
    )
    benchmark_parser.add_argument(
        "--all", action="store_true", help="Test all configured models"
    )
    benchmark_parser.add_argument(
        "--num-images",
        type=int,
        default=None,
        help="Number of test images (default: all data)",
    )
    benchmark_parser.add_argument(
        "--visualize",
        action="store_true",
        help="Enable detection box visualization",
    )
    benchmark_parser.add_argument(
        "--num-viz-images",
        type=int,
        default=10,
        help="Number of visualization images (default: 10)",
    )
    benchmark_parser.add_argument(
        "--conf-threshold",
        type=float,
        default=None,
        help="Confidence threshold (default: use config file value)",
    )
    benchmark_parser.add_argument(
        "--output-dir",
        type=str,
        default="outputs/results",
        help="Output directory (default: outputs/results)",
    )

    # Analyze command
    analyze_parser = subparsers.add_parser(
        "analyze",
        help="Compare baseline model with user model",
        description="Compare baseline model performance with user custom model",
    )
    analyze_parser.add_argument(
        "--baseline",
        type=str,
        action="append",
        help="Baseline model name(s) from config.yaml (can be used multiple times)",
    )
    analyze_parser.add_argument(
        "--all-baselines",
        action="store_true",
        help="Use all configured baseline models",
    )
    analyze_parser.add_argument(
        "--user-model",
        type=str,
        action="append",
        help="User model(s)",
    )
    analyze_parser.add_argument(
        "--config",
        type=str,
        default="config.yaml",
        help="Configuration file path",
    )
    analyze_parser.add_argument(
        "--num-images",
        type=int,
        default=50,
        help="Number of test images",
    )
    analyze_parser.add_argument(
        "--output-dir",
        type=str,
        default="outputs/analysis",
        help="Output directory",
    )
    analyze_parser.add_argument(
        "--format",
        type=str,
        default="all",
        choices=["json", "html", "csv", "all"],
        help="Output format",
    )
    analyze_parser.add_argument(
        "--debug",
        action="store_true",
        help="Debug mode",
    )

    # Export command
    export_parser = subparsers.add_parser(
        "export",
        help="Export model to ONNX or TensorRT format",
        description="Export YOLO models to ONNX or TensorRT format for optimized inference",
    )
    export_parser.add_argument(
        "--model",
        type=str,
        action="append",
        help="Path to model weights file(s) (.pt), can be used multiple times",
    )
    export_parser.add_argument(
        "--all-models",
        action="store_true",
        help="Export all models from models_cache directory",
    )
    export_parser.add_argument(
        "--format",
        type=str,
        default="onnx",
        choices=["onnx", "tensorrt", "all"],
        help="Export format (default: onnx)",
    )
    export_parser.add_argument(
        "--output-dir",
        type=str,
        default="models_export",
        help="Output directory (default: models_export)",
    )
    export_parser.add_argument(
        "--input-size",
        type=int,
        nargs=2,
        default=[640, 640],
        metavar=("H", "W"),
        help="Input image size (default: 640 640)",
    )
    export_parser.add_argument(
        "--dynamic",
        action="store_true",
        help="Use dynamic input size (ONNX only)",
    )
    export_parser.add_argument(
        "--simplify",
        action="store_true",
        default=True,
        help="Simplify ONNX model (default: True)",
    )
    export_parser.add_argument(
        "--fp16",
        action="store_true",
        default=True,
        help="Use FP16 precision (TensorRT only, default: True)",
    )
    export_parser.add_argument(
        "--int8",
        action="store_true",
        help="Use INT8 quantization (TensorRT only)",
    )
    export_parser.add_argument(
        "--batch-size",
        type=int,
        default=1,
        help="Batch size for export (default: 1)",
    )
    export_parser.add_argument(
        "--device",
        type=str,
        default="cpu",
        help="Device for export (default: cpu)",
    )

    # Compare command
    compare_parser = subparsers.add_parser(
        "compare",
        help="Compare model performance across different formats (PyTorch vs ONNX)",
        description="Compare model performance across different formats",
    )
    compare_parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="Path to model weights file (.pt)",
    )
    compare_parser.add_argument(
        "--model-name",
        type=str,
        default=None,
        help="Model name (default: auto-detect from filename)",
    )
    compare_parser.add_argument(
        "--formats",
        type=str,
        default="pytorch,onnx",
        help="Formats to compare, comma-separated (default: pytorch,onnx)",
    )
    compare_parser.add_argument(
        "--config",
        type=str,
        default="config.yaml",
        help="Configuration file path (default: config.yaml)",
    )
    compare_parser.add_argument(
        "--num-images",
        type=int,
        default=50,
        help="Number of test images (default: 50)",
    )
    compare_parser.add_argument(
        "--output-dir",
        type=str,
        default="outputs/format_comparison",
        help="Output directory (default: outputs/format_comparison)",
    )

    args = parser.parse_args()

    if args.command is None:
        parser.print_help()
        return

    if args.command == "benchmark":
        benchmark_main()

    elif args.command == "analyze":
        analyze_main(args)

    elif args.command == "export":
        # å‚æ•°éªŒè¯ï¼šå¿…é¡»æŒ‡å®š --model æˆ– --all-models
        if not args.model and not args.all_models:
            print("é”™è¯¯: å¿…é¡»æŒ‡å®š --model æˆ– --all-models")
            print("ç¤ºä¾‹:")
            print("  od-benchmark export --model model.pt")
            print("  od-benchmark export --model model1.pt --model model2.pt")
            print("  od-benchmark export --all-models")
            return

        batch_export_models(
            model_paths=args.model or [],
            all_models=args.all_models,
            format=args.format,
            output_dir=args.output_dir,
            input_size=tuple(args.input_size),
            dynamic=args.dynamic,
            simplify=args.simplify,
            fp16=args.fp16,
            int8=args.int8,
            batch_size=args.batch_size,
            device=args.device,
        )

    elif args.command == "compare":
        from src.analysis import compare_model_formats_cli

        formats = [f.strip() for f in args.formats.split(",")]
        compare_model_formats_cli(
            model_path=args.model,
            model_name=args.model_name,
            formats=formats,
            config=args.config,
            num_images=args.num_images,
            output_dir=args.output_dir,
        )
    else:
        parser.print_help()
        return


if __name__ == "__main__":
    main()
